{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21851dac",
   "metadata": {},
   "source": [
    "# Scraping Bookstoscrape.com ---> SECTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89b639",
   "metadata": {},
   "source": [
    "### CourseWork 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a479b8",
   "metadata": {},
   "source": [
    "#### Specified Tasks:\n",
    "\n",
    "In this question, you will build a dataset on books. The data will be collected from\n",
    "https://books.toscrape.com/\n",
    "Note: The purpose of this exercise is to demonstrate data acquisition (scraping), cleaning, and exploration.\n",
    "You can use BeautifulSoup and Requests Python libraries.\n",
    "\n",
    "Write scripts to fulfil the below requirements:\n",
    "\n",
    "a. Collect 1000 (or as many as possible) items from the website and save them to\n",
    "csv(s) files.\n",
    "\n",
    "    The data collected should include:\n",
    "    1. Title\n",
    "    2. Rating\n",
    "    3. Price\n",
    "    4. Stock availability (Boolean)\n",
    "    5. UPC\n",
    "    6. Quantity available\n",
    "    7. Category\n",
    "    (10 marks)\n",
    "    \n",
    "b. Identify any problems with the data and clean accordingly. (5 marks)\n",
    "\n",
    "c. Calculate the total value of each category. Which category is the most valuable? (5 marks)\n",
    "\n",
    "d. Which category with more than 10 titles has the highest percentage of books\n",
    "rated 4 stars or higher? (5 marks)\n",
    "\n",
    "e. Compare the average ratings of each category. What are your conclusions? (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65c3af",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be8479",
   "metadata": {},
   "source": [
    "#### STEP 1: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5253acb",
   "metadata": {},
   "source": [
    "    Pandas ---> Our library for data wrangling, data manipulation and data transformation.\n",
    "    Requests ---> Our library for making requests to web pages and accessing resources on the internet.\n",
    "    Time ---> Helps to handle time complexities in our code. Very useful for keeping track of when we last scraped.\n",
    "    BS4 ---> The beautiful soup library used to parse, prettify, and work with HTML tags while scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b2dbf",
   "metadata": {},
   "source": [
    "#### STEP 2: Creating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b595c",
   "metadata": {},
   "source": [
    "To scrape the books from the web page, we break down the task into 4 parts. each of these parts becomes a function that helps make solving the overall problem easier. This process is commonly referred to as functional programming and we would be applying this method towards creating and saving our dataset.\n",
    "\n",
    "###### The Functions:\n",
    "Functions for scraping:\n",
    "- get_info_internal_links -> This function scrapes the html tags from the web address and creates an instance of the beautiful soup library using the data scraped and the 'html.parser' to parse the data.\n",
    "- get_books_into_dataset -> Store all the data collected from each page into a list of lists.\n",
    "- save_scraped_data -> Save the data to the generated_data folder.\n",
    "- iterate_through_pages -> Iterate through multiple and single pages scraping the data from https://books.toscrape.com/ by combining all the functions created.\n",
    "\n",
    "\n",
    "Functions for working with the data gotten:\n",
    "- eda -> Takes a dataset and performs exploratory data analysis.\n",
    "- convert_stock_to_boolean -> This function is created for the availability column in the dataset. It takes each row in the availability column and converts it to boolean TRUE if the book is in stock or boolean FALSE if it is not.\n",
    "- convert_rating_to_int -> This function is created for the rating column in the dataset. It takes each row in the ratings column and converts it to its numerical representation.\n",
    "- get_number_of_books -> This function is applied on a dataset to get the total number of books for each category.\n",
    "- percentage_above_or_equal_to_4rating -> This function is applied on a dataset to get the percentage of books with 4 star rating and above for each category. This function uses the get_number_of_books function to get the numbers of books which will be used to create this function.\n",
    "\n",
    "Please note: The functions used when working with the data only work after the data has been created and the data has been successfully stored as a pandas dataframe and python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fbeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_internal_links(address: str, tag: str, class_: str = None):\n",
    "    \"\"\"\n",
    "    This function scrapes the html tags from the web address and creates an \n",
    "    instance of the beautiful soup library using the data scraped and the \n",
    "    'html.parser' to parse the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    address : str\n",
    "        The website to scrape the data.\n",
    "    tag : str\n",
    "        HTML tag for the container where the books are.\n",
    "    class_ : str, optional\n",
    "        The class associated with the HTML tag. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    info : Tag Object\n",
    "        HTML tags associated with the request.\n",
    "\n",
    "    \"\"\"\n",
    "    scraper = requests.get(address, timeout = 10)\n",
    "    response = scraper.text\n",
    "    \n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    info = soup.find(tag, class_ = class_)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_into_dataset(container_books, books) -> list:\n",
    "    \"\"\"\n",
    "    Store all the data collected from each page into a list of lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    container_books : Tag Object\n",
    "        The HTML tag container where all the books on that page are stored.\n",
    "    books : Tag Object\n",
    "        The HTML tag container where all single book are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    store : list\n",
    "        A list that lists the information of each book as a single row.\n",
    "\n",
    "    \"\"\"\n",
    "    store = []\n",
    "    for each_book in books:\n",
    "        book_title = each_book.h3.a[\"title\"]\n",
    "        book_rating = each_book.p.attrs[\"class\"][1]\n",
    "        book_price = each_book.find(\"p\", class_ =\"price_color\").text.replace(\"Â£\", '')\n",
    "        info_address = each_book.find(\"h3\").a[\"href\"].replace(\"../../../\", \"https://books.toscrape.com/catalogue/\")\n",
    "        \n",
    "        get_upc = get_info_internal_links(address = info_address, tag = \"table\", class_ =\"table table-striped\")\n",
    "        book_upc = get_upc.find(\"td\").text\n",
    "        \n",
    "        book_in_stock = each_book.find(\"p\", class_ =\"instock availability\").text.strip()\n",
    "        \n",
    "        get_quantity_available = get_info_internal_links(address = info_address, tag = \"p\", class_ =\"instock availability\")\n",
    "        book_quantity_available = get_quantity_available.text.strip().replace(\"In stock (\", \"\").split()[0]\n",
    "        \n",
    "        book_category = container_books.find(\"h1\").text\n",
    "        book_more_info = each_book.find(\"h3\").a[\"href\"].replace(\"../../../\", \"https://books.toscrape.com/catalogue/\")\n",
    "        store.append([book_title, \n",
    "                      book_rating, \n",
    "                      book_price, \n",
    "                      book_upc, \n",
    "                      book_in_stock,\n",
    "                      book_quantity_available,\n",
    "                      book_category, \n",
    "                      book_more_info])\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d976cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scraped_data(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save the data to the generated_data folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Dataset containing the scraped data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_name = \"scraped_books_data\" \n",
    "        date = time.strftime(\"%Y-%m-%d\")\n",
    "        dataset.to_csv(f\"../../generated_data/{data_name}_{date}.csv\", index = False)\n",
    "        print(\"\\nSuccessfully saved file to the specified folder ---> generated_data folder.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nFailed to save file to the specified folder ---> generated_data folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_through_pages() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape the data from https://books.toscrape.com/ by combining all the \n",
    "    functions created:\n",
    "        a) get_info_internal_links -> To get the webpage and parse it.\n",
    "        b) get_books_into_dataset -> Gets the book data from the parsed data \n",
    "    and stores it.\n",
    "        c) save_scraped_data -> Save the data to the generated_data folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : pd.DataFrame\n",
    "        A dataframe containing the scraped books data from all pages on the \n",
    "        https://books.toscrape.com/ web page.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the dataset\n",
    "    dataset = pd.DataFrame(columns = [\"Title\", \"Rating\", \"Price\", \"UPC\", \"Availability\", \"Quantity_Available\", \"Category\", \"More_Info\"])\n",
    "    \n",
    "    url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
    "    box = get_info_internal_links(address = url, tag = \"ul\", class_ = \"nav nav-list\").li\n",
    "    column_box = box.find_all(\"li\")\n",
    "    \n",
    "    columns = [column.text.lower().strip().replace(\" \", '-') for column in column_box]\n",
    "    index = list(range(2, len(columns) + 2))\n",
    "    \n",
    "    print(\"Now iterating through pages to scrape data...\\n    TRUE: Categories with next page.\\n    FALSE: Categories with one page.\\n\")\n",
    "    for category, index in zip(columns, index):\n",
    "        url1 = f\"https://books.toscrape.com/catalogue/category/books/{category}_{index}/index.html\"\n",
    "        \n",
    "        container_books = get_info_internal_links(address = url1, tag = \"div\", class_ = \"col-sm-8 col-md-9\")\n",
    "        books = container_books.find_all(\"li\", class_ =\"col-xs-6 col-sm-4 col-md-3 col-lg-3\")\n",
    "        \n",
    "        print(f\"{category}{index} -->\", container_books.find(\"li\", class_ = \"current\") is None)\n",
    "        \n",
    "        \n",
    "        if container_books.find(\"li\", class_ = \"current\") is None:\n",
    "            store = get_books_into_dataset(container_books, books)\n",
    "            data = pd.DataFrame(store, columns = [\"Title\", \"Rating\",\"Price\", \"UPC\", \"Availability\", \"Quantity_Available\", \"Category\", \"More_Info\"])\n",
    "            dataset = pd.concat([dataset, data])\n",
    "            \n",
    "        elif container_books.find(\"li\", class_ = \"current\") is not None:\n",
    "            page_number = container_books.find(\"li\", class_ = \"current\").text.split()[-1]\n",
    "            page_number = int(page_number)\n",
    "            for page in range(1, (page_number + 1)):\n",
    "                url2 = f\"https://books.toscrape.com/catalogue/category/books/{category}_{index}/page-{page}.html\"\n",
    "                container_books = get_info_internal_links(address = url2, tag = \"div\", class_ = \"col-sm-8 col-md-9\")\n",
    "                books = container_books.find_all(\"li\", class_ =\"col-xs-6 col-sm-4 col-md-3 col-lg-3\")\n",
    "                \n",
    "                store = get_books_into_dataset(container_books, books)\n",
    "                data = pd.DataFrame(store, columns = [\"Title\", \"Rating\",\"Price\", \"UPC\", \"Availability\", \"Quantity_Available\", \"Category\", \"More_Info\"])\n",
    "                dataset = pd.concat([dataset, data])\n",
    "        \n",
    "        else:\n",
    "            print(\"No class called 'current'.\")\n",
    "            break\n",
    "    \n",
    "    # Save dataset to folder\n",
    "    save_scraped_data(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(dataset: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Dataset to perform EDA.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing different evaluation metrics for exploring the \n",
    "        columns and understanding how values in the dataset are distributed.\n",
    "\n",
    "    \"\"\"\n",
    "    data_unique = {}\n",
    "    data_category_count = {}\n",
    "    dataset.info()\n",
    "    data_head = dataset.head()\n",
    "    data_tail = dataset.tail()\n",
    "    data_mode = dataset.mode().iloc[0]\n",
    "    data_descriptive_stats = dataset.describe()\n",
    "    data_more_descriptive_stats = dataset.describe(include = \"all\")\n",
    "    data_correlation_matrix = dataset.corr(numeric_only = True)\n",
    "    data_distinct_count = dataset.nunique()\n",
    "    data_count_duplicates = dataset.duplicated().sum()\n",
    "    data_count_null = dataset.isnull().sum()\n",
    "    data_total_null = dataset.isnull().sum().sum()\n",
    "    for each_column in dataset.columns:\n",
    "        data_unique[each_column] = dataset[each_column].unique()\n",
    "    for each_column in dataset.select_dtypes(object).columns:\n",
    "        data_category_count[each_column] = dataset[each_column].value_counts()\n",
    "    \n",
    "    result = {\"data_head\": data_head,\n",
    "              \"data_tail\": data_tail,\n",
    "              \"data_mode\": data_mode,\n",
    "              \"data_descriptive_stats\": data_descriptive_stats,\n",
    "              \"data_more_descriptive_stats\": data_more_descriptive_stats,\n",
    "              \"data_correlation_matrix\": data_correlation_matrix,\n",
    "              \"data_distinct_count\": data_distinct_count,\n",
    "              \"data_count_duplicates\": data_count_duplicates,\n",
    "              \"data_count_null\": data_count_null,\n",
    "              \"data_total_null\": data_total_null,\n",
    "              \"data_unique\": data_unique,\n",
    "              \"data_category_count\": data_category_count,\n",
    "              }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stock_to_boolean(row: str) -> bool:\n",
    "    \"\"\"\n",
    "    This function is created for the availability column in the dataset. It takes \n",
    "    each row in the availability column and converts it to boolean TRUE if the book is\n",
    "    in stock or boolean FALSE if it is not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : str\n",
    "        Each row of the availability column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Returns boolean of TRUE if the book is in stock, else FALSE.\n",
    "\n",
    "    \"\"\"\n",
    "    if row.strip() == \"In stock\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rating_to_int(row: str) -> Union[int, float]:\n",
    "    \"\"\"\n",
    "    This function is created for the rating column in the dataset. It takes \n",
    "    each row in the ratings column and converts it to its numerical representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : str\n",
    "        Each row of the ratings column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[int, float]\n",
    "        Returns an integer which is the numerical representation of the ratings\n",
    "        column or np.nan which is of type float.\n",
    "\n",
    "    \"\"\"\n",
    "    if row.strip() == \"One\":\n",
    "        return 1\n",
    "    elif row.strip() == \"Two\":\n",
    "        return 2\n",
    "    elif row.strip() == \"Three\":\n",
    "        return 3\n",
    "    elif row.strip() == \"Four\":\n",
    "        return 4\n",
    "    elif row.strip() == \"Five\":\n",
    "        return 5\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_books(row) -> int:\n",
    "    \"\"\"\n",
    "    This function is applied on a dataset to get the total number of books for\n",
    "    each category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.DataFrame\n",
    "        Takes a pandas dataframe and goes through the rows working with the \n",
    "        category and count columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Returns an integer representing the total number of books for that category.\n",
    "\n",
    "    \"\"\"\n",
    "    category = row[\"Category\"]\n",
    "    number_of_books = int(categories[categories[\"Category\"] == category][\"Count\"].values)\n",
    "    return number_of_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_above_or_equal_to_4rating(row) -> Union[int, float]:\n",
    "    \"\"\"\n",
    "    This function is applied on a dataset to get the percentage of books with 4\n",
    "    star rating and above for each category. This function uses the get_number_of_books\n",
    "    function to get the numbers of books which will be used to create this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.DataFrame\n",
    "        Takes a pandas dataframe and goes through the rows working with the \n",
    "        count columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[int, float]\n",
    "        An integer or floated value specifying the percentage of books with 4\n",
    "        star rating and above for each category.\n",
    "\n",
    "    \"\"\"\n",
    "    number_of_books = get_number_of_books(row)\n",
    "    number = row[\"Count\"]\n",
    "    return (number/number_of_books) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936d675",
   "metadata": {},
   "source": [
    "#### STEP 3: Start the program... Get the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6876459",
   "metadata": {},
   "source": [
    "(A) SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c923320",
   "metadata": {},
   "source": [
    "We use the function iterate_through_pages as our bot which combines other functions we have created such as get_info_internal_links, get_books_into_dataset, and save_scraped_data to scrape the data from the webpage. This is what we want to achieve when we run our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program Starts\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        dataframe = iterate_through_pages()\n",
    "        print(\"\\n\\nNext scraping will be done in 2 hours.\\nPlease wait...\")\n",
    "        time.sleep(7200) # We sleep for 2 hours to allow our bot not to be detected while it keeps running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7413ca6",
   "metadata": {},
   "source": [
    "#### STEP 4: Working on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scraped data\n",
    "dataset = pd.read_csv(\"../../generated_data/scraped_books_data_2024-02-11.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "initial_eda = eda(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5171b4d",
   "metadata": {},
   "source": [
    "(B) SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb49a9",
   "metadata": {},
   "source": [
    "### Issues identified from the dataset\n",
    "1) Convert the STOCK AVAILABILITY column to BOOLEAN\n",
    "2) Convert the RATING column to integer of numeric values \n",
    "3) Convert the price column to floated values\n",
    "4) Convert the quantity column to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Solution 1: Convert the STOCK AVAILABILITY column to BOOLEAN\n",
    "dataset[\"Availability\"] = dataset[\"Availability\"].apply(convert_stock_to_boolean).astype(bool)\n",
    "# ---> Solution 2: Convert the RATING column to integer of numeric values \n",
    "dataset[\"Rating\"] = dataset[\"Rating\"].apply(convert_rating_to_int)\n",
    "# ---> Solution 3: Convert the price column to floated values\n",
    "dataset[\"Price\"] = dataset[\"Price\"].astype(float)\n",
    "# ---> Solution 4: Convert the quantity column to numeric values \n",
    "dataset[\"Quantity_Available\"] = dataset[\"Quantity_Available\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313585ec",
   "metadata": {},
   "source": [
    "(C) SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa4184",
   "metadata": {},
   "source": [
    "### Calculate the total value of each category. Which category is the most valuable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9217c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Calculate the total value of each category.\n",
    "total_value_category = dataset[[\"Price\", \"Category\"]].groupby(\"Category\").sum()\n",
    "total_value_category = total_value_category.reset_index().sort_values(\"Price\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Which category is the most valuable?\n",
    "most_valuable = dataset[[\"Price\", \"Category\"]].groupby(\"Category\").sum()\n",
    "most_valuable = most_valuable.reset_index().sort_values(\"Price\", ascending = False)\n",
    "most_valuable_category = most_valuable.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3707de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Visualising the most valuable category\n",
    "plt.figure(figsize = (30, 10))\n",
    "colormap = plt.cm.OrRd\n",
    "plt.grid(True, alpha = 0.2)\n",
    "container = plt.bar(most_valuable[\"Category\"], \n",
    "                    most_valuable[\"Price\"], \n",
    "                    color = colormap(np.linspace(0, 1, num = len(most_valuable[\"Category\"]))))\n",
    "plt.bar_label(container, \n",
    "              labels = round(most_valuable[\"Price\"], 1), \n",
    "              padding = 10)\n",
    "plt.plot(most_valuable[\"Category\"], \n",
    "         most_valuable[\"Price\"], \n",
    "         marker = \"o\", \n",
    "         linestyle = \"--\", \n",
    "         color = \"orange\", \n",
    "         markersize = 8)\n",
    "plt.title(\"Analyzing the most valuable category.\", pad = 10, fontsize = 20)\n",
    "plt.xlabel(\"Category.\", labelpad= 10)\n",
    "plt.ylabel(\"Price\", labelpad = 10)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894378d",
   "metadata": {},
   "source": [
    "From our analysis, we find out the most valuable category is the DEFAULT category with a total price of $5227.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13992a",
   "metadata": {},
   "source": [
    "(D) SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97575e4f",
   "metadata": {},
   "source": [
    "### Which category with more than 10 titles has the highest percentage of books rated 4 stars or higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Get the number of categories with more than 10 titles - STAGE 1\n",
    "categories = dataset[\"Category\"].groupby(dataset[\"Category\"]).count()\n",
    "categories.name = \"Count\"\n",
    "categories = categories.reset_index()\n",
    "categories = categories[categories[\"Count\"] >= 10]\n",
    "# ---> Get the number of categories with more than 10 titles - STAGE 2\n",
    "columns = categories[\"Category\"].values\n",
    "filtered_data = dataset[dataset[\"Category\"].isin(columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Category with highest percentage of books rated 4 stars or higher?\n",
    "grouped_ratings = filtered_data[[\"Rating\", \"Category\", \"Title\"]].groupby([\"Category\", \"Rating\"]).count().reset_index()\n",
    "grouped_ratings = grouped_ratings[grouped_ratings[\"Rating\"].isin([4, 5])]\n",
    "ratings = grouped_ratings[[\"Category\", \"Title\"]].groupby(\"Category\").sum().reset_index()\n",
    "ratings = ratings.rename({\"Title\": \"Count\"}, axis = 1)\n",
    "ratings[\"Number of books\"] = ratings.apply(get_number_of_books, axis = 1)\n",
    "ratings[\"%Ratings of 4Stars above\"] = ratings.apply(percentage_above_or_equal_to_4rating, axis = 1)\n",
    "category_highest_percent_rating_above4stars = ratings.sort_values(\"%Ratings of 4Stars above\", ascending = False).iloc[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Visualising category with highest percentage of books rated 4 stars or higher\n",
    "ratings = ratings.sort_values(\"%Ratings of 4Stars above\", ascending = False)\n",
    "plt.figure(figsize = (30, 10))\n",
    "colormap = plt.cm.OrRd\n",
    "plt.grid(True, alpha = 0.2)\n",
    "container = plt.bar(ratings[\"Category\"], \n",
    "                    ratings[\"%Ratings of 4Stars above\"], \n",
    "                    color = colormap(np.linspace(0, 1, num = len(ratings[\"Category\"]))))\n",
    "plt.bar_label(container, \n",
    "              labels = round(ratings[\"%Ratings of 4Stars above\"], 1), \n",
    "              padding = 10)\n",
    "plt.plot(ratings[\"Category\"], \n",
    "         ratings[\"%Ratings of 4Stars above\"], \n",
    "         marker = \"o\", \n",
    "         linestyle = \"--\", \n",
    "         color = \"orange\", \n",
    "         markersize = 8)\n",
    "plt.title(\"Analyzing category with highest percentage of books rated 4 stars or higher.\", pad = 10, fontsize = 20)\n",
    "plt.xlabel(\"Category.\", labelpad= 10)\n",
    "plt.ylabel(\"Percentage of Ratings of 4 stars and above.\", labelpad = 10)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59a28c",
   "metadata": {},
   "source": [
    "From our analysis, we find out the category with the highest percentage of books rated 4 stars or higher is the POETRY category. The POETRY category, with a rating of 63.2%, is the category with the highest percentage of books rated 4 stars or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c7297",
   "metadata": {},
   "source": [
    "(E) SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb11ea",
   "metadata": {},
   "source": [
    "### Compare the average ratings of each category. What are your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Getting the average ratings\n",
    "avg_rating = dataset[[\"Rating\", \"Category\"]].groupby(\"Category\").mean(numeric_only = True)\n",
    "avg_rating = avg_rating.reset_index().sort_values(\"Rating\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Visualise average ratings of each category\n",
    "plt.figure(figsize = (30, 10))\n",
    "colormap = plt.cm.OrRd\n",
    "plt.grid(True, alpha = 0.2)\n",
    "container = plt.bar(avg_rating[\"Category\"], \n",
    "                    avg_rating[\"Rating\"], \n",
    "                    color = colormap(np.linspace(0, 1, num = len(avg_rating[\"Category\"]))))\n",
    "plt.bar_label(container, \n",
    "              labels = round(avg_rating[\"Rating\"], 1), \n",
    "              padding = 10)\n",
    "plt.plot(avg_rating[\"Category\"], \n",
    "         avg_rating[\"Rating\"], \n",
    "         marker = \"o\", \n",
    "         linestyle = \"--\", \n",
    "         color = \"orange\", \n",
    "         markersize = 8)\n",
    "plt.title(\"Analyzing average ratings of each category.\", pad = 10, fontsize = 20)\n",
    "plt.xlabel(\"Category.\", labelpad= 10)\n",
    "plt.ylabel(\"Rating.\", labelpad = 10)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcf138",
   "metadata": {},
   "source": [
    "From our analysis, we learn that 3 categories have the highest average rating of\n",
    "5.0 across all books sold, rated, and reviewed. The three categories with the \n",
    "highest average rating of 5.0 are:\n",
    "   - Erotica Category\n",
    "   - Adult Fiction Category\n",
    "   - Novels Category\n",
    "\n",
    "At the lower end, we have 4 categories with a shared lowest rating of 1.0 across\n",
    "all books sold, rated, and reviewed. The four categories with the \n",
    "lowest average rating of 1.0 are:\n",
    "   - Short Stories Category\n",
    "   - Crime Category\n",
    "   - Cultural Category\n",
    "   - Paranormal Category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
